{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll build a sentiment classifier using the review rating as the sentiment gold standard. \n",
    "#Treating this as a binary classification task, we can consider a 1-2 star review as negative, and a 4-5 star review as positive. \n",
    "#In order to achieve a roughly balanced data set, you may want to remove some of the positive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, re\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from random import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import some of the parsing and loading methods we had from previous tasks\n",
    "def parse_label(label):\n",
    "    if label == '__label2__':\n",
    "        return 'real'\n",
    "    else:\n",
    "        return 'fake'    \n",
    "\n",
    "def parse_verification(label):\n",
    "    if label == 'N':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1      \n",
    "\n",
    "def flatten(lst):\n",
    "    for el in lst:\n",
    "        if isinstance(el, list):\n",
    "            yield from el\n",
    "        else:\n",
    "            yield el    \n",
    "\n",
    "# Convert line from input file into an id/text/label tuple\n",
    "def parseReview(reviewLine):\n",
    "    # Should return a triple of an integer, a string containing the review, and a string indicating the label\n",
    "    return (reviewLine[0], \\\n",
    "            reviewLine[8] + ' ' + reviewLine[7], \\\n",
    "            parse_label(reviewLine[1]), \\\n",
    "            reviewLine[2], \\\n",
    "            parse_verification(reviewLine[3]))\n",
    "\n",
    "# load data from a file and append it to the rawData\n",
    "def loadData(path, Text=None):\n",
    "    with open(path) as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        next(reader, None)  # skip the headers        \n",
    "        for line in reader:\n",
    "            (Id, Text, Label, Rating, Verified) = parseReview(line)\n",
    "            rawData.append((Id, Text, Label, Rating, Verified))\n",
    "            \n",
    "#Loading our set\n",
    "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
    "reviewPath = 'amazon_reviews.txt'\n",
    "loadData(reviewPath) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll want to put it all in a dataframe, and add some additional features\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import pyphen\n",
    "dic = pyphen.Pyphen(lang='en')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def averageLen(lst):\n",
    "    lengths = [len(i) for i in lst]\n",
    "    return 0 if len(lengths) == 0 else (float(sum(lengths)) / len(lengths)) \n",
    "\n",
    "\n",
    "names = ['doc_id', 'label', 'rating', 'verified_purchase', \\\n",
    "         'pr_category', 'pr_id', 'pr_title', 'review_title', 'review_text']\n",
    "data = pd.read_csv('amazon_reviews.txt', skiprows=[0], names=names, sep='\\t')\n",
    "data['verified_purchase'] = data['verified_purchase'].apply(lambda x: 1 if x=='Y' else 0)\n",
    "data['label'] = data['label'].apply(lambda x: 'real' if x=='__label2__' else 'fake')\n",
    "data['review_text_length'] = data.apply(lambda row: len(row.review_text), axis=1)\n",
    "data['review_title_length'] = data.apply(lambda row: len(row.review_title), axis=1)\n",
    "data['tokenized_review'] = data.apply(lambda row: re.sub(r\"(\\w)([.,;:!-?'\\\"”\\)])\", r\"\\1 \\2\", row['review_text']), axis=1)\n",
    "data['tokenized_review'] = data.apply(lambda row: re.sub(r\"([.,;:!-?'\\\"“\\(])(\\w)\", r\"\\1 \\2\", row['tokenized_review']), axis=1)\n",
    "data['tokenized_review'] = data.apply(lambda row: re.sub(r\"<[^>]*>\", \"\", row['tokenized_review']), axis=1)\n",
    "data['tokenized_review'] = data.apply(lambda row: nltk.word_tokenize(row['tokenized_review'].lower()), axis=1)\n",
    "data['tokenized_review_string'] = data['tokenized_review'].apply(' '.join)\n",
    "data['tokens'] = data.apply(lambda row: [t for t in row['tokenized_review'] if t.isalpha()], axis=1)\n",
    "data['tokens'] = data.apply(lambda row: [t for t in row['tokens'] if t not in stop_words], axis=1)\n",
    "data['stopwords'] = data.apply(lambda row: [t for t in row['tokenized_review'] if t.isalpha()], axis=1)\n",
    "data['stopwords'] = data.apply(lambda row: [t for t in row['tokenized_review'] if t in stopwords.words('english')], axis=1)\n",
    "data['num_tokens'] = data.apply(lambda row: len(row['tokens']), axis=1)\n",
    "data['avg_len_tokens'] = data.apply(lambda row: averageLen(row['tokens']), axis=1)\n",
    "data['num_stopwords'] = data.apply(lambda row: len(row['stopwords']), axis=1)\n",
    "data['word_count'] = data.apply(lambda row: len(row['tokenized_review']), axis=1)\n",
    "data['sent_count'] = data.apply(lambda row: len(sent_tokenize(row['review_text'])), axis=1)\n",
    "data['syll_count'] = data.apply(lambda row: len(list(flatten([dic.inserted(text).split('-') for text in row['tokenized_review']]))), axis=1)\n",
    "data['flesch_kincaid'] = data.apply(lambda row: 206.835 - 1.015*row['word_count']/row['sent_count'] - 84.6*row['syll_count']/row['word_count'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally on to our classifier, the first thing is to add a column that will serve to capture the sentiment\n",
    "def get_sent(rating):\n",
    "    if rating < 3:\n",
    "        sent = 'negative'\n",
    "    elif rating > 3:\n",
    "        sent = 'positive'\n",
    "    else:\n",
    "        sent = 'mweh'\n",
    "    return sent\n",
    "\n",
    "data['sentiment'] = data.apply(lambda row: get_sent(row['rating']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's make some vectors from our data, these will be used by the classifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['review_text'], data['sentiment'], test_size=0.33, random_state=1)\n",
    "\n",
    "# Initialize a CountVectorizer and Tfidf objects\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.7)\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using only the 'review_text' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy: 80.92%\n",
      "Confusion matrix:\n",
      "[[ 341   16  628]\n",
      " [  42    7  546]\n",
      " [  66   24 5260]]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules for our classifier\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print('Classifier accuracy: ' + str(round(100*score, 2)) + '%')\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['negative', 'mweh', 'positive'])\n",
    "print('Confusion matrix:')\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy: 77.2%\n",
      "Confusion matrix:\n",
      "[[   0    0  985]\n",
      " [   0    0  595]\n",
      " [   0    0 5350]]\n"
     ]
    }
   ],
   "source": [
    "# What if we used to Tfidf vectors instead?\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print('Classifier accuracy: ' + str(round(100*score, 2)) + '%')\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['negative', 'mweh', 'positive'])\n",
    "print('Confusion matrix:')\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.0 score: 0.781\n",
      "Alpha: 0.1 score: 0.792\n",
      "Alpha: 0.2 score: 0.782\n",
      "Alpha: 0.3 score: 0.776\n",
      "Alpha: 0.4 score: 0.775\n",
      "Alpha: 0.5 score: 0.774\n",
      "Alpha: 0.6 score: 0.773\n",
      "Alpha: 0.7 score: 0.773\n",
      "Alpha: 0.8 score: 0.772\n",
      "Alpha: 0.9 score: 0.772\n",
      "Alpha: 1.0 score: 0.772\n",
      "Alpha: 1.1 score: 0.772\n",
      "Alpha: 1.2 score: 0.772\n",
      "Alpha: 1.3 score: 0.772\n",
      "Alpha: 1.4 score: 0.772\n"
     ]
    }
   ],
   "source": [
    "# Create the list of alphas: alphas\n",
    "import numpy as np\n",
    "alphas = np.arange(0, 1.5, .1)\n",
    "\n",
    "def train_and_predict(alpha):\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ' + str(alpha) + ' score: ' + str(round(train_and_predict(alpha), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.0 score: 0.778\n",
      "Alpha: 0.1 score: 0.801\n",
      "Alpha: 0.2 score: 0.806\n",
      "Alpha: 0.3 score: 0.814\n",
      "Alpha: 0.4 score: 0.815\n",
      "Alpha: 0.5 score: 0.818\n",
      "Alpha: 0.6 score: 0.817\n",
      "Alpha: 0.7 score: 0.816\n",
      "Alpha: 0.8 score: 0.814\n",
      "Alpha: 0.9 score: 0.812\n",
      "Alpha: 1.0 score: 0.809\n",
      "Alpha: 1.1 score: 0.806\n",
      "Alpha: 1.2 score: 0.804\n",
      "Alpha: 1.3 score: 0.803\n",
      "Alpha: 1.4 score: 0.802\n"
     ]
    }
   ],
   "source": [
    "alphas = np.arange(0, 1.5, .1)\n",
    "\n",
    "def train_and_predict(alpha):\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    nb_classifier.fit(count_train, y_train)\n",
    "    pred = nb_classifier.predict(count_test)\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ' + str(alpha) + ' score: ' + str(round(train_and_predict(alpha), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams only:\n",
      "Classifier accuracy: 81.76%\n",
      "Confusion matrix:\n",
      "[[ 466   40  479]\n",
      " [  82   28  485]\n",
      " [ 112   66 5172]]\n",
      "Ngrams:\n",
      "Classifier accuracy: 83.62%\n",
      "Confusion matrix:\n",
      "[[ 542   37  406]\n",
      " [ 102   33  460]\n",
      " [  88   42 5220]]\n"
     ]
    }
   ],
   "source": [
    "#But we can also play with vectorizing ngrams instead of just unigrams\n",
    "print('Unigrams only:')\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "nb_classifier = MultinomialNB(alpha=0.5)\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "pred = nb_classifier.predict(count_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print('Classifier accuracy: ' + str(round(100*score, 2)) + '%')\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['negative', 'mweh', 'positive'])\n",
    "print('Confusion matrix:')\n",
    "print(cm)\n",
    "\n",
    "print('Ngrams:')\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 7), analyzer='char')\n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "nb_classifier = MultinomialNB(alpha=0.2)\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "pred = nb_classifier.predict(count_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print('Classifier accuracy: ' + str(round(100*score, 2)) + '%')\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['negative', 'mweh', 'positive'])\n",
    "print('Confusion matrix:')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy: 83.3%\n",
      "Confusion matrix:\n",
      "[[ 624  104  257]\n",
      " [ 122   96  377]\n",
      " [ 156  141 5053]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 7), analyzer='char')\n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "svm_classifier = LinearSVC()\n",
    "# nb_classifier = MultinomialNB(alpha=0.2)\n",
    "svm_classifier.fit(count_train, y_train)\n",
    "# nb_classifier.fit(count_train, y_train)\n",
    "# pred = nb_classifier.predict(count_test)\n",
    "pred = svm_classifier.predict(count_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print('Classifier accuracy: ' + str(round(100*score, 2)) + '%')\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['negative', 'mweh', 'positive'])\n",
    "print('Confusion matrix:')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [6930, 14070]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-c529cfb7bd75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Classifier accuracy: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'negative'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mweh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'positive'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mdiffering_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [6930, 14070]"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.\n",
    "\n",
    "    The data is expected to be stored in a 2D data structure, where the first\n",
    "    index is over features and the second is over samples.  i.e.\n",
    "\n",
    "    >> len(data[key]) == n_samples\n",
    "\n",
    "    Please note that this is the opposite convention to scikit-learn feature\n",
    "    matrixes (where the first index corresponds to sample).\n",
    "\n",
    "    ItemSelector only requires that the collection implement getitem\n",
    "    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n",
    "    DataFrame, numpy record array, etc.\n",
    "\n",
    "    >> data = {'a': [1, 5, 2, 5, 2, 8],\n",
    "               'b': [9, 4, 1, 4, 1, 3]}\n",
    "    >> ds = ItemSelector(key='a')\n",
    "    >> data['a'] == ds.transform(data)\n",
    "\n",
    "    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n",
    "    list of dicts).  If your data is structured this way, consider a\n",
    "    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    # Use FeatureUnion to combine the features from subject and body\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "            ('review_text', Pipeline([\n",
    "                ('selector', ItemSelector(key='review_text')),\n",
    "                ('tfidf', CountVectorizer()),\n",
    "            ])),\n",
    "            ('rating', Pipeline([\n",
    "                ('selector', ItemSelector(key='rating')),\n",
    "                ('tfidf', CountVectorizer())\n",
    "            ])),\n",
    "            ('verified_purchase', Pipeline([\n",
    "                ('selector', ItemSelector(key='verified_purchase')),\n",
    "                ('stats', CountVectorizer())\n",
    "            ])),\n",
    "\n",
    "        ]\n",
    "    )),\n",
    "    ('classifier', MultinomialNB(alpha=0.2)),\n",
    "])\n",
    "\n",
    "\n",
    "data_2 = data[['review_text', 'rating', 'verified_purchase', 'sentiment']]\n",
    "data_2['rating'] = data.apply(lambda row: 'rating_' + str(row['rating']), axis=1)\n",
    "data_2['verified_purchase'] = data.apply(lambda row: 'verified_' + str(row['verified_purchase']), axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_2[['review_text', 'rating', 'verified_purchase']], data_2['sentiment'], test_size=0.33, random_state=1)\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "pred = pipeline.predict(X_train)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print('Classifier accuracy: ' + str(round(100*score, 2)) + '%')\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['negative', 'mweh', 'positive'])\n",
    "print('Confusion matrix:')\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
