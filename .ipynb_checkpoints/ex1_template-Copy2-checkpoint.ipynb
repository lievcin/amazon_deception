{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from random import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "import re, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from a file and append it to the rawData\n",
    "def loadData(path, Text=None):\n",
    "    with open(path) as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        next(reader, None)  # skip the headers\n",
    "        for line in reader:\n",
    "            (Id, Text, Label) = parseReview(line)\n",
    "            rawData.append((Id, Text, Label))\n",
    "            preprocessedData.append((Id, toFeatureVector(preProcess(Text)), Label))\n",
    "        \n",
    "def splitData(percentage):\n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    for (_, Text, Label) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainData.append((_, toFeatureVector(preProcess(Text)),Label))\n",
    "    for (_, Text, Label) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testData.append((_, toFeatureVector(preProcess(Text)),Label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 1\n",
    "def parse_label(label):\n",
    "    if label == '__label2__':\n",
    "        return 'real'\n",
    "    else:\n",
    "        return 'fake'    \n",
    "\n",
    "# Convert line from input file into an id/text/label tuple\n",
    "def parseReview(reviewLine):\n",
    "    # Should return a triple of an integer, a string containing the review, and a string indicating the label\n",
    "    return reviewLine[0], reviewLine[8], parse_label(reviewLine[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO!!!!!!!\n",
    "# TEXT PREPROCESSING AND FEATURE VECTORIZATION\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus.reader.wordnet import NOUN\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.classify import SklearnClassifier\n",
    "\n",
    "\n",
    "#So that we can download wordnet, one-off\n",
    "# import ssl\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "s = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "l = WordNetLemmatizer()\n",
    "translator=str.maketrans('','',string.punctuation)\n",
    "\n",
    "# Input: a string of one review\n",
    "def preProcess(text):\n",
    "    text = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", text)\n",
    "    text = re.sub(r\"([.,;:!?'\\\"“\\(])(\\w)\", r\"\\1 \\2\", text)    \n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = text.translate(translator)\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "#     tokens = [s.stem(t) for t in tokens]\n",
    "    tokens = [l.lemmatize(t) for t in tokens]    \n",
    "    tokens = [t for t in tokens if t not in stop]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 2\n",
    "featureDict = {} # A global dictionary of features\n",
    "\n",
    "def toFeatureVector(tokens):\n",
    "    # Should return a dictionary containing features as keys, and weights as values\n",
    "    v = {}\n",
    "    for t in tokens:\n",
    "        try:\n",
    "            featureDict[t] += 1\n",
    "        except KeyError:            \n",
    "            featureDict[t] = 1\n",
    "        try:\n",
    "            v[t] += (1.0/len(tokens))\n",
    "        except KeyError:\n",
    "            v[t] = (1.0/len(tokens))\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC())])\n",
    "    return SklearnClassifier(pipeline).train(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 3\n",
    "\n",
    "def crossValidate(dataset, folds):\n",
    "    shuffle(dataset)\n",
    "    cv_results = []\n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    foldSize = int(len(dataset)/folds)\n",
    "    \n",
    "    for i in range(0,len(dataset), foldSize):\n",
    "        trainFolds = []\n",
    "        validationFold = []\n",
    "        trainFolds = dataset[:i] + dataset[i+foldSize:]        \n",
    "        validationFold = dataset[i: i+foldSize]\n",
    "        \n",
    "        training_set = [(t[1], t[2]) for t in trainFolds]\n",
    "        classifier = trainClassifier(training_set)\n",
    "        validation_set = [(t[0], t[1]) for t in validationFold]\n",
    "        predictions.append(predictLabels(validationFold, classifier))\n",
    "        ground_truth.append([ l[2] for l in validationFold])\n",
    "\n",
    "    return ground_truth, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
    "\n",
    "def predictLabels(reviewSamples, classifier):\n",
    "    return classifier.classify_many([t[1] for t in reviewSamples])\n",
    "#     return classifier.classify_many(map(lambda t: toFeatureVector(preProcess(t[1])), reviewSamples))\n",
    "\n",
    "def predictLabel(reviewSample, classifier):\n",
    "    return classifier.classify(toFeatureVector(preProcess(reviewSample[1])))\n",
    "\n",
    "def flatten(lst):\n",
    "    for el in lst:\n",
    "        if isinstance(el, list):\n",
    "            yield from el\n",
    "        else:\n",
    "            yield el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "Preparing the dataset...\n",
      "Now 21000 rawData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Now 21000 rawData, 0 trainData, 0 testData\n",
      "Training Samples: \n",
      "0\n",
      "Features: \n",
      "65512\n",
      "Accuracy: 64.81%\n",
      "Confusion Matrix: \n",
      "TP: 7132\n",
      "TN: 6478\n",
      "FP: 4022\n",
      "FN: 3368\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "# loading reviews\n",
    "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
    "preprocessedData = [] # the preprocessed reviews (just to see how your preprocessing is doing)\n",
    "trainData = []        # the training data as a percentage of the total dataset (currently 80%, or 16800 samples)\n",
    "testData = []         # the test data as a percentage of the total dataset (currently 20%, or 4200 samples)\n",
    "\n",
    "# the output classes\n",
    "fakeLabel = 'fake'\n",
    "realLabel = 'real'\n",
    "\n",
    "# references to the data files\n",
    "reviewPath = 'amazon_reviews.txt'\n",
    "\n",
    "## Do the actual stuff\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing the dataset...\",sep='\\n')\n",
    "loadData(reviewPath) \n",
    "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "# splitData(1)\n",
    "\n",
    "ground_truth, predictions = crossValidate(preprocessedData, 10)\n",
    "ground_truth = list(flatten(ground_truth))\n",
    "predictions = list(flatten(predictions))\n",
    "\n",
    "# We print the number of training samples and the number of features\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Training Samples: \", len(trainData), \"Features: \", len(featureDict), sep='\\n')\n",
    "print('Accuracy: ' + str(round(100*accuracy_score(ground_truth, predictions), 2)) + '%')\n",
    "print('Confusion Matrix: ')\n",
    "tn, fp, fn, tp = confusion_matrix(ground_truth, predictions).ravel()\n",
    "print('TP: ' + str(tp))\n",
    "print('TN: ' + str(tn))\n",
    "print('FP: ' + str(fp))\n",
    "print('FN: ' + str(fn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = predictLabels(testData, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('5508', 'This one pound diaphragm pump is easy to mount and is fantastic in its output. It manages to pump more than a gallon of water per minute while drawing only 2 amperes. The structure looks sturdy and the lifetime is guaranteed to be more than five years. It is also able to run dry without any visible damage to its structure or function.', 'fake')]\n",
      "('5508', {'one': 0.030303030303030304, 'pound': 0.030303030303030304, 'diaphragm': 0.030303030303030304, 'pump': 0.06060606060606061, 'easy': 0.030303030303030304, 'mount': 0.030303030303030304, 'fantastic': 0.030303030303030304, 'output': 0.030303030303030304, 'manages': 0.030303030303030304, 'gallon': 0.030303030303030304, 'water': 0.030303030303030304, 'per': 0.030303030303030304, 'minute': 0.030303030303030304, 'drawing': 0.030303030303030304, '2': 0.030303030303030304, 'ampere': 0.030303030303030304, 'structure': 0.06060606060606061, 'look': 0.030303030303030304, 'sturdy': 0.030303030303030304, 'lifetime': 0.030303030303030304, 'guaranteed': 0.030303030303030304, 'five': 0.030303030303030304, 'year': 0.030303030303030304, 'also': 0.030303030303030304, 'able': 0.030303030303030304, 'run': 0.030303030303030304, 'dry': 0.030303030303030304, 'without': 0.030303030303030304, 'visible': 0.030303030303030304, 'damage': 0.030303030303030304, 'function': 0.030303030303030304}, 'fake')\n"
     ]
    }
   ],
   "source": [
    "print([rec for rec in rawData if rec[0]=='5508'])\n",
    "print(preprocessedData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rawData2 = []\n",
    "# trainData2 = []\n",
    "# testData2 = []\n",
    "# reviewPath = 'amazon_reviews.txt'\n",
    "\n",
    "# def loadData2(path, Text=None):\n",
    "#     with open(path) as f:\n",
    "#         reader = csv.reader(f, delimiter='\\t')\n",
    "#         next(reader, None)  # skip the headers\n",
    "#         for line in reader:\n",
    "#             (Id, Text, Label) = parseReview(line)\n",
    "#             rawData2.append((Id, Text, Label))\n",
    "#             trainData2.append((toFeatureVector(preProcess(Text)),Label))\n",
    "            \n",
    "# def crossValidate2(dataset, folds):\n",
    "#     shuffle(dataset)\n",
    "#     results = []\n",
    "#     foldSize = int(len(dataset)/folds)\n",
    "#     for i in range(0,len(dataset),foldSize):\n",
    "#         clf = trainClassifier(dataset[:i] + dataset[i+foldSize:])\n",
    "#         pLabel = predictLabels2(dataset[i:i+foldSize], clf)\n",
    "#         yLabel = [ l[1] for l in dataset[i:i+foldSize]]\n",
    "#         results.append(accuracy_score(yLabel, pLabel))\n",
    "#         # insert code here that trains and tests on the 10 folds of data in the dataset\n",
    "#     return results\n",
    "\n",
    "# def predictLabels2(tweetData, classifier):\n",
    "#     test = map(lambda t: t[0], tweetData)\n",
    "#     return classifier.classify_many(test)\n",
    "\n",
    "# loadData2(reviewPath) \n",
    "# print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData2), len(trainData2), len(testData2)),\n",
    "#       \"Preparing training and test data...\",sep='\\n')\n",
    "# cv_results = crossValidate2(trainData2, 10)\n",
    "# print(sum(cv_results)/len(cv_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['And', 'now', 'for', 'something', 'completely', 'different']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = word_tokenize('And now for something completely different')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
